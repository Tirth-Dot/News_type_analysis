{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7ea7ac-99fc-4839-88c2-2fd75c8cc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Tirth\\Projects\\Module11 (NLP)\n",
      "\n",
      "Files in current directory:\n",
      "Module11 nlp questions.pdf\n",
      "News_Category_Dataset_v3.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for item in os.listdir('.'):\n",
    "    if 'Module11' in item or 'News' in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ef8c72-dd56-4be7-aaa3-0431fc28b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (209527, 6)\n",
      "\n",
      "Columns: ['link', 'headline', 'category', 'short_description', 'authors', 'date']\n",
      "\n",
      "First few rows:\n",
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "        date  \n",
      "0 2022-09-23  \n",
      "1 2022-09-23  \n",
      "2 2022-09-23  \n",
      "3 2022-09-23  \n",
      "4 2022-09-22  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "\n",
    "print(f\"Shape of dataframe: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2a6e71-e265-41a9-be3e-3c3ad3f2200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 42\n",
      "\n",
      "All categories:\n",
      "['U.S. NEWS' 'COMEDY' 'PARENTING' 'WORLD NEWS' 'CULTURE & ARTS' 'TECH'\n",
      " 'SPORTS' 'ENTERTAINMENT' 'POLITICS' 'WEIRD NEWS' 'ENVIRONMENT'\n",
      " 'EDUCATION' 'CRIME' 'SCIENCE' 'WELLNESS' 'BUSINESS' 'STYLE & BEAUTY'\n",
      " 'FOOD & DRINK' 'MEDIA' 'QUEER VOICES' 'HOME & LIVING' 'WOMEN'\n",
      " 'BLACK VOICES' 'TRAVEL' 'MONEY' 'RELIGION' 'LATINO VOICES' 'IMPACT'\n",
      " 'WEDDINGS' 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'THE WORLDPOST' 'GOOD NEWS' 'WORLDPOST' 'FIFTY' 'ARTS'\n",
      " 'DIVORCE']\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "CRIME              3562\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "SCIENCE            2206\n",
      "TECH               2104\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "U.S. NEWS          1377\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "EDUCATION          1014\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find number of unique categories\n",
    "num_categories = df['category'].nunique()\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "print(f\"\\nAll categories:\")\n",
    "print(df['category'].unique())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd5d6bf-9eb3-47bf-a9c9-75feecfff601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 209527 rows\n",
      "After removing ['WEIRD NEWS', 'IMPACT', 'DIVORCE']:\n",
      "Filtered dataset: 199840 rows\n",
      "Rows removed: 9687\n",
      "\n",
      "Remaining categories: 39\n"
     ]
    }
   ],
   "source": [
    "# Define which categories you want to keep or remove\n",
    "\n",
    "unwanted_categories = ['WEIRD NEWS', 'IMPACT', 'DIVORCE']\n",
    "\n",
    "# Create a filtered dataframe\n",
    "df_filtered = df[~df['category'].isin(unwanted_categories)]\n",
    "\n",
    "print(f\"Original dataset: {len(df)} rows\")\n",
    "print(f\"After removing {unwanted_categories}:\")\n",
    "print(f\"Filtered dataset: {len(df_filtered)} rows\")\n",
    "print(f\"Rows removed: {len(df) - len(df_filtered)}\")\n",
    "print(f\"\\nRemaining categories: {df_filtered['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6068ac49-6cfe-400c-a58f-6f1125716735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 209527 rows\n",
      "Keeping only: ['TECHNO', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
      "Filtered dataset: 58956 rows\n",
      "Rows removed: 150571\n",
      "\n",
      "Categories in filtered data:\n",
      "category\n",
      "POLITICS         35602\n",
      "ENTERTAINMENT    17362\n",
      "BUSINESS          5992\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# METHOD 2: Keep only specific categories\n",
    "# Whitelist approach - define categories you WANT\n",
    "\n",
    "keep_categories = ['TECHNO', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
    "\n",
    "df_keep = df[df['category'].isin(keep_categories)]\n",
    "\n",
    "print(f\"Original dataset: {len(df)} rows\")\n",
    "print(f\"Keeping only: {keep_categories}\")\n",
    "print(f\"Filtered dataset: {len(df_keep)} rows\")\n",
    "print(f\"Rows removed: {len(df) - len(df_keep)}\")\n",
    "print(f\"\\nCategories in filtered data:\")\n",
    "print(df_keep['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cdb4eb-34da-4c95-8dd6-cf1dc01985a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.huffpost.com/entry/golden-globes-r...</td>\n",
       "      <td>Golden Globes Returning To NBC In January Afte...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>For the past 18 months, Hollywood has effectiv...</td>\n",
       "      <td></td>\n",
       "      <td>2022-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.huffpost.com/entry/biden-us-forces...</td>\n",
       "      <td>Biden Says U.S. Forces Would Defend Taiwan If ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>President issues vow as tensions with China rise.</td>\n",
       "      <td></td>\n",
       "      <td>2022-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.huffpost.com/entry/ukraine-festiva...</td>\n",
       "      <td>â€˜Beautiful And Sad At The Same Timeâ€™: Ukrainia...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>An annual celebration took on a different feel...</td>\n",
       "      <td>Jonathan Nicholson</td>\n",
       "      <td>2022-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.huffpost.com/entry/james-cameron-f...</td>\n",
       "      <td>James Cameron Says He 'Clashed' With Studio Be...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>The \"Avatar\" director said aspects of his 2009...</td>\n",
       "      <td>Ben Blanchet</td>\n",
       "      <td>2022-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.huffpost.com/entry/europe-britain-...</td>\n",
       "      <td>Biden Says Queen's Death Left 'Giant Hole' For...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>U.S. President Joe Biden, in London for the fu...</td>\n",
       "      <td>Darlene Superville, AP</td>\n",
       "      <td>2022-09-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 link  \\\n",
       "20  https://www.huffpost.com/entry/golden-globes-r...   \n",
       "21  https://www.huffpost.com/entry/biden-us-forces...   \n",
       "24  https://www.huffpost.com/entry/ukraine-festiva...   \n",
       "28  https://www.huffpost.com/entry/james-cameron-f...   \n",
       "30  https://www.huffpost.com/entry/europe-britain-...   \n",
       "\n",
       "                                             headline       category  \\\n",
       "20  Golden Globes Returning To NBC In January Afte...  ENTERTAINMENT   \n",
       "21  Biden Says U.S. Forces Would Defend Taiwan If ...       POLITICS   \n",
       "24  â€˜Beautiful And Sad At The Same Timeâ€™: Ukrainia...       POLITICS   \n",
       "28  James Cameron Says He 'Clashed' With Studio Be...  ENTERTAINMENT   \n",
       "30  Biden Says Queen's Death Left 'Giant Hole' For...       POLITICS   \n",
       "\n",
       "                                    short_description                 authors  \\\n",
       "20  For the past 18 months, Hollywood has effectiv...                           \n",
       "21  President issues vow as tensions with China rise.                           \n",
       "24  An annual celebration took on a different feel...      Jonathan Nicholson   \n",
       "28  The \"Avatar\" director said aspects of his 2009...            Ben Blanchet   \n",
       "30  U.S. President Joe Biden, in London for the fu...  Darlene Superville, AP   \n",
       "\n",
       "         date  \n",
       "20 2022-09-20  \n",
       "21 2022-09-19  \n",
       "24 2022-09-19  \n",
       "28 2022-09-18  \n",
       "30 2022-09-18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4a5bce-bfc6-4ee3-b73c-74257d924b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_sm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1a1f92-b1b3-4deb-846c-87ec2e0de1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPACY LOADING METHODS - OPTIMIZED FOR PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "1. Load ONLY tokenizer (bare minimum):\n",
      "   Active components: ['tok2vec', 'lemmatizer']\n",
      "   Use case: Fast tokenization, text preprocessing\n",
      "   Memory: ~10 MB\n",
      "\n",
      "2. Load tokenizer + POS tagger:\n",
      "   Active components: ['tok2vec', 'tagger', 'lemmatizer']\n",
      "   Use case: POS tagging, grammar analysis\n",
      "   Memory: ~35 MB\n",
      "\n",
      "3. Load tokenizer + tagger + parser (no NER):\n",
      "   Active components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
      "   Use case: Dependency parsing, sentence structure\n",
      "   Memory: ~40 MB\n",
      "\n",
      "4. Load full model (all components):\n",
      "   Active components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "   Use case: NER, complete NLP analysis\n",
      "\n",
      "Input: Apple Inc. is buying U.K. startup for $1 billion.\n",
      "Tokens: ['Apple', 'Inc.', 'is', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python apps\\anaconda\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPACY LOADING METHODS - OPTIMIZED FOR PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Method 1: Load ONLY tokenizer (fastest)\n",
    "print(\"\\n1. Load ONLY tokenizer (bare minimum):\")\n",
    "nlp_tokenizer = spacy.load('en_core_web_sm', \n",
    "                           disable=['tagger', 'parser', 'ner', 'attribute_ruler'])\n",
    "print(f\"   Active components: {nlp_tokenizer.pipe_names}\")\n",
    "print(f\"   Use case: Fast tokenization, text preprocessing\")\n",
    "print(f\"   Memory: ~10 MB\")\n",
    "\n",
    "# Method 2: Load tokenizer + tagger\n",
    "print(\"\\n2. Load tokenizer + POS tagger:\")\n",
    "nlp_tagger = spacy.load('en_core_web_sm', \n",
    "                       disable=['parser', 'ner', 'attribute_ruler'])\n",
    "print(f\"   Active components: {nlp_tagger.pipe_names}\")\n",
    "print(f\"   Use case: POS tagging, grammar analysis\")\n",
    "print(f\"   Memory: ~35 MB\")\n",
    "\n",
    "# Method 3: Load with parser\n",
    "print(\"\\n3. Load tokenizer + tagger + parser (no NER):\")\n",
    "nlp_parser = spacy.load('en_core_web_sm', \n",
    "                       disable=['ner'])\n",
    "print(f\"   Active components: {nlp_parser.pipe_names}\")\n",
    "print(f\"   Use case: Dependency parsing, sentence structure\")\n",
    "print(f\"   Memory: ~40 MB\")\n",
    "\n",
    "# Method 4: Full model\n",
    "print(\"\\n4. Load full model (all components):\")\n",
    "nlp_full = spacy.load('en_core_web_sm')\n",
    "print(f\"   Active components: {nlp_full.pipe_names}\")\n",
    "print(f\"   Use case: NER, complete NLP analysis\")\n",
    "\n",
    "text = \"Apple Inc. is buying U.K. startup for $1 billion.\"\n",
    "doc = nlp_tokenizer(text)\n",
    "print(f\"\\nInput: {text}\")\n",
    "print(f\"Tokens: {[token.text for token in doc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402e09f9-c9a1-44e9-b70d-403d96523536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "print(\"âœ“ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69b65ea-ca53-4df1-abca-3342a1064b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HEADLINE PREPROCESSING - TEST EXAMPLES\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_headlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m test_headlines \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple Inc. is looking at buying British startup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Quick Brown Fox Jumps Over The Lazy Dog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Ways to Stay Healthy and Fit in 2024\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, headline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_headlines, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     cleaned \u001b[38;5;241m=\u001b[39m preprocess_headlines(headline)\n\u001b[0;32m     15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m preprocess_headlines_tokens(headline)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Original:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_headlines' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HEADLINE PREPROCESSING - TEST EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_headlines = [\n",
    "    \"Apple Inc. is looking at buying British startup\",\n",
    "    \"The Quick Brown Fox Jumps Over The Lazy Dog\",\n",
    "    \"COVID-19 Vaccines: What You Need to Know!\",\n",
    "    \"Breaking News: Markets Rally as Economic Data Improves\",\n",
    "    \"Top 10 Ways to Stay Healthy and Fit in 2024\"\n",
    "]\n",
    "\n",
    "for i, headline in enumerate(test_headlines, 1):\n",
    "    cleaned = preprocess_headlines(headline)\n",
    "    tokens = preprocess_headlines_tokens(headline)\n",
    "    \n",
    "    print(f\"\\n{i}. Original:\")\n",
    "    print(f\"   {headline}\")\n",
    "    print(f\"\\n   Cleaned (string):\")\n",
    "    print(f\"   {cleaned}\")\n",
    "    print(f\"\\n   Tokens (list):\")\n",
    "    print(f\"   {tokens}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898ae87-6e37-4734-a450-653e0ef2f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_headlines(headline):\n",
    "    \"\"\"\n",
    "    Preprocess a headline: lowercase, remove punctuation, and lemmatize\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = headline.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else '' for char in text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_headlines_tokens(headline):\n",
    "    \"\"\"\n",
    "    Preprocess and tokenize headline using spaCy\n",
    "    Returns list of lemmatized tokens (excluding stopwords)\n",
    "    \"\"\"\n",
    "    # Preprocess first\n",
    "    cleaned = preprocess_headlines(headline)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(cleaned)\n",
    "    \n",
    "    # Extract lemmatized tokens, excluding stopwords and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cefacb-aac6-40f8-80a8-47c4a64d848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: FILTER DATA TO 4 SPECIFIC CATEGORIES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: FILTERING DATA TO 4 CATEGORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categories_to_keep = ['TECH', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
    "\n",
    "# Filter dataframe to keep only these categories\n",
    "df_filtered = df[df['category'].isin(categories_to_keep)].copy()\n",
    "\n",
    "print(f\"\\nOriginal dataframe shape: {df.shape}\")\n",
    "print(f\"Filtered dataframe shape: {df_filtered.shape}\")\n",
    "print(f\"\\nCategories in filtered data:\")\n",
    "print(df_filtered['category'].value_counts())\n",
    "print(f\"\\nTotal records: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# STEP 2: TEXT PREPROCESSING\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TEXT PREPROCESSING - CLEAN HEADLINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by converting to lowercase and removing special characters\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else '' for char in text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"\\nApplying text preprocessing to all headlines...\")\n",
    "df_filtered['cleaned_headline'] = df_filtered['headline'].apply(clean_text)\n",
    "\n",
    "print(f\"Total headlines processed: {len(df_filtered)}\")\n",
    "print(\"\\nSample of original vs cleaned headlines:\")\n",
    "for i in range(min(3, len(df_filtered))):\n",
    "    print(f\"\\nOriginal [{i}]: {df_filtered['headline'].iloc[i][:80]}...\")\n",
    "    print(f\"Cleaned  [{i}]: {df_filtered['cleaned_headline'].iloc[i][:80]}...\")\n",
    "    \n",
    "\n",
    "# STEP 3: TEXT VECTORIZATION WITH COUNTVECTORIZER\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: TEXT VECTORIZATION WITH COUNTVECTORIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "print(\"\\nVectorizer Configuration:\")\n",
    "print(\"  - Max features: 5000\")\n",
    "print(\"  - N-gram range: (1, 2) - unigrams and bigrams\")\n",
    "print(\"  - Min document frequency: 2\")\n",
    "print(\"  - Max document frequency: 0.8\")\n",
    "print(\"  - Stop words: English\")\n",
    "\n",
    "print(\"\\nFitting vectorizer and transforming text...\")\n",
    "X = vectorizer.fit_transform(df_filtered['cleaned_headline'])\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Total features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nSample features (first 20):\")\n",
    "print(vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "\n",
    "# STEP 4: CREATE FEATURE MATRIX (X) AND LABEL VECTOR (y)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: CREATE FEATURE MATRIX AND LABEL VECTOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y = df_filtered['category'].values\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Label vector y shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution in data:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for cat, count in zip(unique, counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  {cat}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# STEP 5: TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: TRAIN-TEST SPLIT WITH STRATIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cat, count in zip(unique_train, counts_train):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "    \n",
    "\n",
    "# STEP 6: TRAIN LOGISTIC REGRESSION MODEL\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: TRAIN LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training completed!\")\n",
    "print(f\"Model classes: {model.classes_}\")\n",
    "print(f\"Number of features: {model.n_features_in_}\")\n",
    "\n",
    "# STEP 7: EVALUATE MODEL ON TEST SET\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "import pandas as pd\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)\n",
    "print(cm_df)\n",
    "\n",
    "\n",
    "# STEP 8: PREDICTION FUNCTION FOR NEW HEADLINES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: PREDICTION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_headline_category(headline):\n",
    "    \"\"\"\n",
    "    Predict the category of a given headline using the trained model.\n",
    "    Uses only data from the loaded dataset - no test data.\n",
    "    \"\"\"\n",
    "    cleaned = clean_text(headline)\n",
    "    X_new = vectorizer.transform([cleaned])\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    confidence = model.predict_proba(X_new)[0].max()\n",
    "    return prediction, confidence\n",
    "\n",
    "# Test with actual headlines from df_filtered\n",
    "print(\"\\nTesting prediction function with actual headlines from dataset:\")\n",
    "print(\"\\nSample predictions from actual data:\")\n",
    "test_indices = np.random.choice(len(df_filtered), size=min(5, len(df_filtered)), replace=False)\n",
    "\n",
    "for idx in test_indices:\n",
    "    actual_headline = df_filtered['headline'].iloc[idx]\n",
    "    actual_category = df_filtered['category'].iloc[idx]\n",
    "    predicted_category, confidence = predict_headline_category(actual_headline)\n",
    "    \n",
    "    match = \"âœ“ CORRECT\" if predicted_category == actual_category else \"âœ— INCORRECT\"\n",
    "    print(f\"\\nHeadline: {actual_headline[:70]}...\")\n",
    "    print(f\"Actual: {actual_category} | Predicted: {predicted_category} [{confidence:.2%}] {match}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel successfully trained on {len(df_filtered)} news articles\")\n",
    "print(f\"Test set accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"\\nModel is ready for predictions on new headlines.\")\n",
    "print(f\"Use: predict_headline_category('Your headline here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dbfe9-e4f6-4c6d-951b-990e51543e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE CHATBOT - NEWS CATEGORY PREDICTOR\n",
    "# ============================================================================\n",
    "\n",
    "def run_interactive_chatbot():\n",
    "    \"\"\"\n",
    "    Interactive chatbot for predicting news headline categories.\n",
    "    Run this to chat with the model!\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“° WELCOME TO NEWS CATEGORY CHATBOT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nI can predict the category of any news headline!\")\n",
    "    print(f\"\\nAvailable categories: {', '.join(model.classes_)}\")\n",
    "    print(\"\\nType 'quit', 'exit', or 'q' to end the conversation.\\n\")\n",
    "    \n",
    "    conversation_count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_headline = input(\"ðŸ“ Enter a news headline (or 'quit' to exit): \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_headline.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nðŸ‘‹ Thank you for using NewsBot! Goodbye!\")\n",
    "                print(f\"Total headlines processed: {conversation_count}\")\n",
    "                break\n",
    "            \n",
    "            # Validate input\n",
    "            if not user_headline:\n",
    "                print(\"âš ï¸  Please enter a valid headline.\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_category, confidence = predict_headline_category(user_headline)\n",
    "            conversation_count += 1\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\nâœ… Prediction: {predicted_category}\")\n",
    "            print(f\"   Confidence: {confidence:.2%}\")\n",
    "            \n",
    "            # Confidence indicator\n",
    "            if confidence > 0.9:\n",
    "                reliability = \"ðŸŸ¢ Very High (>90%)\"\n",
    "            elif confidence > 0.8:\n",
    "                reliability = \"ðŸŸ¢ High (80-90%)\"\n",
    "            elif confidence > 0.7:\n",
    "                reliability = \"ðŸŸ¡ Medium (70-80%)\"\n",
    "            else:\n",
    "                reliability = \"ðŸ”´ Low (<70%)\"\n",
    "            \n",
    "            print(f\"   Reliability: {reliability}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nðŸ‘‹ Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            print(\"Please try again.\\n\")\n",
    "\n",
    "# RUN THE CHATBOT\n",
    "run_interactive_chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1c79f-af08-49ff-a080-c9edbc75b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170de18-e877-45a2-bc4a-f2609575914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# assume these exist from your training:\n",
    "# vectorizer = CountVectorizer(...)\n",
    "# model = LogisticRegression(...)\n",
    "\n",
    "# after model.fit(X_train_vec, y_train)\n",
    "with open(\"news_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open(\"news_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
