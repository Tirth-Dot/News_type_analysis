{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7ea7ac-99fc-4839-88c2-2fd75c8cc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Tirth\\Projects\\Module11 (NLP)\n",
      "\n",
      "Files in current directory:\n",
      "Module11 nlp questions.pdf\n",
      "News_Category_Dataset_v3.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check current working directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# List files in current directory to find the JSON file\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for item in os.listdir('.'):\n",
    "    if 'Module11' in item or 'News' in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ef8c72-dd56-4be7-aaa3-0431fc28b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (209527, 6)\n",
      "\n",
      "Columns: ['link', 'headline', 'category', 'short_description', 'authors', 'date']\n",
      "\n",
      "First few rows:\n",
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "        date  \n",
      "0 2022-09-23  \n",
      "1 2022-09-23  \n",
      "2 2022-09-23  \n",
      "3 2022-09-23  \n",
      "4 2022-09-22  \n"
     ]
    }
   ],
   "source": [
    "# Now read the JSON file (lines=True for JSON Lines format)\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "\n",
    "print(f\"Shape of dataframe: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2a6e71-e265-41a9-be3e-3c3ad3f2200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 42\n",
      "\n",
      "All categories:\n",
      "['U.S. NEWS' 'COMEDY' 'PARENTING' 'WORLD NEWS' 'CULTURE & ARTS' 'TECH'\n",
      " 'SPORTS' 'ENTERTAINMENT' 'POLITICS' 'WEIRD NEWS' 'ENVIRONMENT'\n",
      " 'EDUCATION' 'CRIME' 'SCIENCE' 'WELLNESS' 'BUSINESS' 'STYLE & BEAUTY'\n",
      " 'FOOD & DRINK' 'MEDIA' 'QUEER VOICES' 'HOME & LIVING' 'WOMEN'\n",
      " 'BLACK VOICES' 'TRAVEL' 'MONEY' 'RELIGION' 'LATINO VOICES' 'IMPACT'\n",
      " 'WEDDINGS' 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'THE WORLDPOST' 'GOOD NEWS' 'WORLDPOST' 'FIFTY' 'ARTS'\n",
      " 'DIVORCE']\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "CRIME              3562\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "SCIENCE            2206\n",
      "TECH               2104\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "U.S. NEWS          1377\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "EDUCATION          1014\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find number of unique categories\n",
    "num_categories = df['category'].nunique()\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "print(f\"\\nAll categories:\")\n",
    "print(df['category'].unique())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd5d6bf-9eb3-47bf-a9c9-75feecfff601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 209527 rows\n",
      "After removing ['WEIRD NEWS', 'IMPACT', 'DIVORCE']:\n",
      "Filtered dataset: 199840 rows\n",
      "Rows removed: 9687\n",
      "\n",
      "Remaining categories: 39\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: Remove specific unwanted categories\n",
    "# Define which categories you want to keep or remove\n",
    "\n",
    "unwanted_categories = ['WEIRD NEWS', 'IMPACT', 'DIVORCE']\n",
    "\n",
    "# Create a filtered dataframe\n",
    "df_filtered = df[~df['category'].isin(unwanted_categories)]\n",
    "\n",
    "print(f\"Original dataset: {len(df)} rows\")\n",
    "print(f\"After removing {unwanted_categories}:\")\n",
    "print(f\"Filtered dataset: {len(df_filtered)} rows\")\n",
    "print(f\"Rows removed: {len(df) - len(df_filtered)}\")\n",
    "print(f\"\\nRemaining categories: {df_filtered['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6068ac49-6cfe-400c-a58f-6f1125716735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 209527 rows\n",
      "Keeping only: ['TECHNO', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
      "Filtered dataset: 58956 rows\n",
      "Rows removed: 150571\n",
      "\n",
      "Categories in filtered data:\n",
      "category\n",
      "POLITICS         35602\n",
      "ENTERTAINMENT    17362\n",
      "BUSINESS          5992\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# METHOD 2: Keep only specific categories\n",
    "# Whitelist approach - define categories you WANT\n",
    "\n",
    "keep_categories = ['TECHNO', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
    "\n",
    "df_keep = df[df['category'].isin(keep_categories)]\n",
    "\n",
    "print(f\"Original dataset: {len(df)} rows\")\n",
    "print(f\"Keeping only: {keep_categories}\")\n",
    "print(f\"Filtered dataset: {len(df_keep)} rows\")\n",
    "print(f\"Rows removed: {len(df) - len(df_keep)}\")\n",
    "print(f\"\\nCategories in filtered data:\")\n",
    "print(df_keep['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cdb4eb-34da-4c95-8dd6-cf1dc01985a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.huffpost.com/entry/golden-globes-r...</td>\n",
       "      <td>Golden Globes Returning To NBC In January Afte...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>For the past 18 months, Hollywood has effectiv...</td>\n",
       "      <td></td>\n",
       "      <td>2022-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.huffpost.com/entry/biden-us-forces...</td>\n",
       "      <td>Biden Says U.S. Forces Would Defend Taiwan If ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>President issues vow as tensions with China rise.</td>\n",
       "      <td></td>\n",
       "      <td>2022-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.huffpost.com/entry/ukraine-festiva...</td>\n",
       "      <td>‚ÄòBeautiful And Sad At The Same Time‚Äô: Ukrainia...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>An annual celebration took on a different feel...</td>\n",
       "      <td>Jonathan Nicholson</td>\n",
       "      <td>2022-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.huffpost.com/entry/james-cameron-f...</td>\n",
       "      <td>James Cameron Says He 'Clashed' With Studio Be...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>The \"Avatar\" director said aspects of his 2009...</td>\n",
       "      <td>Ben Blanchet</td>\n",
       "      <td>2022-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.huffpost.com/entry/europe-britain-...</td>\n",
       "      <td>Biden Says Queen's Death Left 'Giant Hole' For...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>U.S. President Joe Biden, in London for the fu...</td>\n",
       "      <td>Darlene Superville, AP</td>\n",
       "      <td>2022-09-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 link  \\\n",
       "20  https://www.huffpost.com/entry/golden-globes-r...   \n",
       "21  https://www.huffpost.com/entry/biden-us-forces...   \n",
       "24  https://www.huffpost.com/entry/ukraine-festiva...   \n",
       "28  https://www.huffpost.com/entry/james-cameron-f...   \n",
       "30  https://www.huffpost.com/entry/europe-britain-...   \n",
       "\n",
       "                                             headline       category  \\\n",
       "20  Golden Globes Returning To NBC In January Afte...  ENTERTAINMENT   \n",
       "21  Biden Says U.S. Forces Would Defend Taiwan If ...       POLITICS   \n",
       "24  ‚ÄòBeautiful And Sad At The Same Time‚Äô: Ukrainia...       POLITICS   \n",
       "28  James Cameron Says He 'Clashed' With Studio Be...  ENTERTAINMENT   \n",
       "30  Biden Says Queen's Death Left 'Giant Hole' For...       POLITICS   \n",
       "\n",
       "                                    short_description                 authors  \\\n",
       "20  For the past 18 months, Hollywood has effectiv...                           \n",
       "21  President issues vow as tensions with China rise.                           \n",
       "24  An annual celebration took on a different feel...      Jonathan Nicholson   \n",
       "28  The \"Avatar\" director said aspects of his 2009...            Ben Blanchet   \n",
       "30  U.S. President Joe Biden, in London for the fu...  Darlene Superville, AP   \n",
       "\n",
       "         date  \n",
       "20 2022-09-20  \n",
       "21 2022-09-19  \n",
       "24 2022-09-19  \n",
       "28 2022-09-18  \n",
       "30 2022-09-18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4a5bce-bfc6-4ee3-b73c-74257d924b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úì Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "print(\"‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1a1f92-b1b3-4deb-846c-87ec2e0de1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPACY LOADING METHODS - OPTIMIZED FOR PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "1. Load ONLY tokenizer (bare minimum):\n",
      "   Active components: ['tok2vec', 'lemmatizer']\n",
      "   Use case: Fast tokenization, text preprocessing\n",
      "   Memory: ~10 MB\n",
      "\n",
      "2. Load tokenizer + POS tagger:\n",
      "   Active components: ['tok2vec', 'tagger', 'lemmatizer']\n",
      "   Use case: POS tagging, grammar analysis\n",
      "   Memory: ~35 MB\n",
      "\n",
      "3. Load tokenizer + tagger + parser (no NER):\n",
      "   Active components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
      "   Use case: Dependency parsing, sentence structure\n",
      "   Memory: ~40 MB\n",
      "\n",
      "4. Load full model (all components):\n",
      "   Active components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "   Use case: NER, complete NLP analysis\n",
      "   Memory: ~45 MB\n",
      "\n",
      "======================================================================\n",
      "TEST: Processing text with minimal components\n",
      "======================================================================\n",
      "\n",
      "Input: Apple Inc. is buying U.K. startup for $1 billion.\n",
      "Tokens: ['Apple', 'Inc.', 'is', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python apps\\anaconda\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPACY LOADING METHODS - OPTIMIZED FOR PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Method 1: Load ONLY tokenizer (fastest)\n",
    "print(\"\\n1. Load ONLY tokenizer (bare minimum):\")\n",
    "nlp_tokenizer = spacy.load('en_core_web_sm', \n",
    "                           disable=['tagger', 'parser', 'ner', 'attribute_ruler'])\n",
    "print(f\"   Active components: {nlp_tokenizer.pipe_names}\")\n",
    "print(f\"   Use case: Fast tokenization, text preprocessing\")\n",
    "print(f\"   Memory: ~10 MB\")\n",
    "\n",
    "# Method 2: Load tokenizer + tagger\n",
    "print(\"\\n2. Load tokenizer + POS tagger:\")\n",
    "nlp_tagger = spacy.load('en_core_web_sm', \n",
    "                       disable=['parser', 'ner', 'attribute_ruler'])\n",
    "print(f\"   Active components: {nlp_tagger.pipe_names}\")\n",
    "print(f\"   Use case: POS tagging, grammar analysis\")\n",
    "print(f\"   Memory: ~35 MB\")\n",
    "\n",
    "# Method 3: Load with parser\n",
    "print(\"\\n3. Load tokenizer + tagger + parser (no NER):\")\n",
    "nlp_parser = spacy.load('en_core_web_sm', \n",
    "                       disable=['ner'])\n",
    "print(f\"   Active components: {nlp_parser.pipe_names}\")\n",
    "print(f\"   Use case: Dependency parsing, sentence structure\")\n",
    "print(f\"   Memory: ~40 MB\")\n",
    "\n",
    "# Method 4: Full model\n",
    "print(\"\\n4. Load full model (all components):\")\n",
    "nlp_full = spacy.load('en_core_web_sm')\n",
    "print(f\"   Active components: {nlp_full.pipe_names}\")\n",
    "print(f\"   Use case: NER, complete NLP analysis\")\n",
    "print(f\"   Memory: ~45 MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST: Processing text with minimal components\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "text = \"Apple Inc. is buying U.K. startup for $1 billion.\"\n",
    "doc = nlp_tokenizer(text)\n",
    "print(f\"\\nInput: {text}\")\n",
    "print(f\"Tokens: {[token.text for token in doc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402e09f9-c9a1-44e9-b70d-403d96523536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK for stopwords (if not already installed)\n",
    "!pip install -q nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "print(\"‚úì Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69b65ea-ca53-4df1-abca-3342a1064b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HEADLINE PREPROCESSING - TEST EXAMPLES\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_headlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m test_headlines \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple Inc. is looking at buying British startup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Quick Brown Fox Jumps Over The Lazy Dog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Ways to Stay Healthy and Fit in 2024\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, headline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_headlines, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     cleaned \u001b[38;5;241m=\u001b[39m preprocess_headlines(headline)\n\u001b[0;32m     16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m preprocess_headlines_tokens(headline)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Original:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_headlines' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing function\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HEADLINE PREPROCESSING - TEST EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_headlines = [\n",
    "    \"Apple Inc. is looking at buying British startup\",\n",
    "    \"The Quick Brown Fox Jumps Over The Lazy Dog\",\n",
    "    \"COVID-19 Vaccines: What You Need to Know!\",\n",
    "    \"Breaking News: Markets Rally as Economic Data Improves\",\n",
    "    \"Top 10 Ways to Stay Healthy and Fit in 2024\"\n",
    "]\n",
    "\n",
    "for i, headline in enumerate(test_headlines, 1):\n",
    "    cleaned = preprocess_headlines(headline)\n",
    "    tokens = preprocess_headlines_tokens(headline)\n",
    "    \n",
    "    print(f\"\\n{i}. Original:\")\n",
    "    print(f\"   {headline}\")\n",
    "    print(f\"\\n   Cleaned (string):\")\n",
    "    print(f\"   {cleaned}\")\n",
    "    print(f\"\\n   Tokens (list):\")\n",
    "    print(f\"   {tokens}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4898ae87-6e37-4734-a450-653e0ef2f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_headlines(headline):\n",
    "    \"\"\"\n",
    "    Preprocess a headline: lowercase, remove punctuation, and lemmatize\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = headline.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else '' for char in text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_headlines_tokens(headline):\n",
    "    \"\"\"\n",
    "    Preprocess and tokenize headline using spaCy\n",
    "    Returns list of lemmatized tokens (excluding stopwords)\n",
    "    \"\"\"\n",
    "    # Preprocess first\n",
    "    cleaned = preprocess_headlines(headline)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(cleaned)\n",
    "    \n",
    "    # Extract lemmatized tokens, excluding stopwords and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27cefacb-aac6-40f8-80a8-47c4a64d848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: FILTERING DATA TO 4 CATEGORIES\n",
      "================================================================================\n",
      "\n",
      "Original dataframe shape: (209527, 6)\n",
      "Filtered dataframe shape: (61060, 6)\n",
      "\n",
      "Categories in filtered data:\n",
      "category\n",
      "POLITICS         35602\n",
      "ENTERTAINMENT    17362\n",
      "BUSINESS          5992\n",
      "TECH              2104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 61060\n",
      "\n",
      "================================================================================\n",
      "STEP 2: TEXT PREPROCESSING - CLEAN HEADLINES\n",
      "================================================================================\n",
      "\n",
      "Applying text preprocessing to all headlines...\n",
      "Total headlines processed: 61060\n",
      "\n",
      "Sample of original vs cleaned headlines:\n",
      "\n",
      "Original [0]: Twitch Bans Gambling Sites After Streamer Scams Folks Out Of $200,000...\n",
      "Cleaned  [0]: twitch bans gambling sites after streamer scams folks out of 200000...\n",
      "\n",
      "Original [1]: Golden Globes Returning To NBC In January After Year Off-Air...\n",
      "Cleaned  [1]: golden globes returning to nbc in january after year offair...\n",
      "\n",
      "Original [2]: Biden Says U.S. Forces Would Defend Taiwan If China Invaded...\n",
      "Cleaned  [2]: biden says us forces would defend taiwan if china invaded...\n",
      "\n",
      "================================================================================\n",
      "STEP 3: TEXT VECTORIZATION WITH COUNTVECTORIZER\n",
      "================================================================================\n",
      "\n",
      "Vectorizer Configuration:\n",
      "  - Max features: 5000\n",
      "  - N-gram range: (1, 2) - unigrams and bigrams\n",
      "  - Min document frequency: 2\n",
      "  - Max document frequency: 0.8\n",
      "  - Stop words: English\n",
      "\n",
      "Fitting vectorizer and transforming text...\n",
      "\n",
      "Feature matrix shape: (61060, 5000)\n",
      "Total features (vocabulary size): 5000\n",
      "\n",
      "Sample features (first 20):\n",
      "['10' '10 things' '10 years' '100' '100 days' '100 million' '1000'\n",
      " '100000' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '20 years'\n",
      " '200']\n",
      "\n",
      "================================================================================\n",
      "STEP 4: CREATE FEATURE MATRIX AND LABEL VECTOR\n",
      "================================================================================\n",
      "\n",
      "Feature matrix X shape: (61060, 5000)\n",
      "Label vector y shape: (61060,)\n",
      "\n",
      "Class distribution in data:\n",
      "  BUSINESS: 5992 (9.8%)\n",
      "  ENTERTAINMENT: 17362 (28.4%)\n",
      "  POLITICS: 35602 (58.3%)\n",
      "  TECH: 2104 (3.4%)\n",
      "\n",
      "================================================================================\n",
      "STEP 5: TRAIN-TEST SPLIT WITH STRATIFICATION\n",
      "================================================================================\n",
      "\n",
      "Training set: 48848 samples, 5000 features\n",
      "Test set: 12212 samples, 5000 features\n",
      "\n",
      "Class distribution in training set:\n",
      "  BUSINESS: 4794\n",
      "  ENTERTAINMENT: 13890\n",
      "  POLITICS: 28481\n",
      "  TECH: 1683\n",
      "\n",
      "================================================================================\n",
      "STEP 6: TRAIN LOGISTIC REGRESSION MODEL\n",
      "================================================================================\n",
      "\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python apps\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training completed!\n",
      "Model classes: ['BUSINESS' 'ENTERTAINMENT' 'POLITICS' 'TECH']\n",
      "Number of features: 5000\n",
      "\n",
      "================================================================================\n",
      "STEP 7: MODEL EVALUATION ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Model Accuracy: 0.8751 (87.51%)\n",
      "Precision (weighted): 0.8706\n",
      "Recall (weighted): 0.8751\n",
      "F1-Score (weighted): 0.8713\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLASSIFICATION REPORT\n",
      "--------------------------------------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BUSINESS       0.71      0.59      0.64      1198\n",
      "ENTERTAINMENT       0.89      0.88      0.88      3472\n",
      "     POLITICS       0.90      0.94      0.92      7121\n",
      "         TECH       0.75      0.52      0.62       421\n",
      "\n",
      "     accuracy                           0.88     12212\n",
      "    macro avg       0.81      0.73      0.77     12212\n",
      " weighted avg       0.87      0.88      0.87     12212\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "               BUSINESS  ENTERTAINMENT  POLITICS  TECH\n",
      "BUSINESS            711            106       352    29\n",
      "ENTERTAINMENT        64           3051       346    11\n",
      "POLITICS            167            217      6705    32\n",
      "TECH                 65             58        78   220\n",
      "\n",
      "================================================================================\n",
      "STEP 8: PREDICTION FUNCTION\n",
      "================================================================================\n",
      "\n",
      "Testing prediction function with actual headlines from dataset:\n",
      "\n",
      "Sample predictions from actual data:\n",
      "\n",
      "Headline: Hailee Steinfeld's Empowering First Solo Single Is Here...\n",
      "Actual: ENTERTAINMENT | Predicted: ENTERTAINMENT [87.51%] ‚úì CORRECT\n",
      "\n",
      "Headline: Wednesday's Morning Email: Understanding What's At Stake As Brexit Vot...\n",
      "Actual: POLITICS | Predicted: POLITICS [99.12%] ‚úì CORRECT\n",
      "\n",
      "Headline: Mark Meadows Says Jan. 6 Panel Leaked Texts To 'Vilify' Him...\n",
      "Actual: POLITICS | Predicted: POLITICS [76.79%] ‚úì CORRECT\n",
      "\n",
      "Headline: See Emma Watson's Glorious 'Harry Potter' Reunion With Costars...\n",
      "Actual: ENTERTAINMENT | Predicted: ENTERTAINMENT [99.98%] ‚úì CORRECT\n",
      "\n",
      "Headline: Beyonc√© Lost To 'Grease: Live' At The Emmys, And You Can Guess What Ha...\n",
      "Actual: ENTERTAINMENT | Predicted: ENTERTAINMENT [99.46%] ‚úì CORRECT\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Model successfully trained on 61060 news articles\n",
      "Test set accuracy: 87.51%\n",
      "\n",
      "Model is ready for predictions on new headlines.\n",
      "Use: predict_headline_category('Your headline here')\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE NLP PIPELINE - NEWS CATEGORY CLASSIFICATION\n",
    "# ============================================================================\n",
    "# This code processes the loaded dataset (df) and builds a news category\n",
    "# classification model using Logistic Regression with CountVectorizer.\n",
    "# NO TEST DATA - only uses the actual loaded df dataset\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: FILTER DATA TO 4 SPECIFIC CATEGORIES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: FILTERING DATA TO 4 CATEGORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categories_to_keep = ['TECH', 'ENTERTAINMENT', 'POLITICS', 'BUSINESS']\n",
    "\n",
    "# Filter dataframe to keep only these categories\n",
    "df_filtered = df[df['category'].isin(categories_to_keep)].copy()\n",
    "\n",
    "print(f\"\\nOriginal dataframe shape: {df.shape}\")\n",
    "print(f\"Filtered dataframe shape: {df_filtered.shape}\")\n",
    "print(f\"\\nCategories in filtered data:\")\n",
    "print(df_filtered['category'].value_counts())\n",
    "print(f\"\\nTotal records: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# STEP 2: TEXT PREPROCESSING\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TEXT PREPROCESSING - CLEAN HEADLINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by converting to lowercase and removing special characters\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else '' for char in text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"\\nApplying text preprocessing to all headlines...\")\n",
    "df_filtered['cleaned_headline'] = df_filtered['headline'].apply(clean_text)\n",
    "\n",
    "print(f\"Total headlines processed: {len(df_filtered)}\")\n",
    "print(\"\\nSample of original vs cleaned headlines:\")\n",
    "for i in range(min(3, len(df_filtered))):\n",
    "    print(f\"\\nOriginal [{i}]: {df_filtered['headline'].iloc[i][:80]}...\")\n",
    "    print(f\"Cleaned  [{i}]: {df_filtered['cleaned_headline'].iloc[i][:80]}...\")\n",
    "    \n",
    "\n",
    "# STEP 3: TEXT VECTORIZATION WITH COUNTVECTORIZER\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: TEXT VECTORIZATION WITH COUNTVECTORIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "print(\"\\nVectorizer Configuration:\")\n",
    "print(\"  - Max features: 5000\")\n",
    "print(\"  - N-gram range: (1, 2) - unigrams and bigrams\")\n",
    "print(\"  - Min document frequency: 2\")\n",
    "print(\"  - Max document frequency: 0.8\")\n",
    "print(\"  - Stop words: English\")\n",
    "\n",
    "print(\"\\nFitting vectorizer and transforming text...\")\n",
    "X = vectorizer.fit_transform(df_filtered['cleaned_headline'])\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Total features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nSample features (first 20):\")\n",
    "print(vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "\n",
    "# STEP 4: CREATE FEATURE MATRIX (X) AND LABEL VECTOR (y)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: CREATE FEATURE MATRIX AND LABEL VECTOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y = df_filtered['category'].values\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Label vector y shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution in data:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for cat, count in zip(unique, counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  {cat}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# STEP 5: TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: TRAIN-TEST SPLIT WITH STRATIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cat, count in zip(unique_train, counts_train):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "    \n",
    "\n",
    "# STEP 6: TRAIN LOGISTIC REGRESSION MODEL\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: TRAIN LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training completed!\")\n",
    "print(f\"Model classes: {model.classes_}\")\n",
    "print(f\"Number of features: {model.n_features_in_}\")\n",
    "\n",
    "# STEP 7: EVALUATE MODEL ON TEST SET\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "import pandas as pd\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)\n",
    "print(cm_df)\n",
    "\n",
    "\n",
    "# STEP 8: PREDICTION FUNCTION FOR NEW HEADLINES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: PREDICTION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_headline_category(headline):\n",
    "    \"\"\"\n",
    "    Predict the category of a given headline using the trained model.\n",
    "    Uses only data from the loaded dataset - no test data.\n",
    "    \"\"\"\n",
    "    cleaned = clean_text(headline)\n",
    "    X_new = vectorizer.transform([cleaned])\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    confidence = model.predict_proba(X_new)[0].max()\n",
    "    return prediction, confidence\n",
    "\n",
    "# Test with actual headlines from df_filtered\n",
    "print(\"\\nTesting prediction function with actual headlines from dataset:\")\n",
    "print(\"\\nSample predictions from actual data:\")\n",
    "test_indices = np.random.choice(len(df_filtered), size=min(5, len(df_filtered)), replace=False)\n",
    "\n",
    "for idx in test_indices:\n",
    "    actual_headline = df_filtered['headline'].iloc[idx]\n",
    "    actual_category = df_filtered['category'].iloc[idx]\n",
    "    predicted_category, confidence = predict_headline_category(actual_headline)\n",
    "    \n",
    "    match = \"‚úì CORRECT\" if predicted_category == actual_category else \"‚úó INCORRECT\"\n",
    "    print(f\"\\nHeadline: {actual_headline[:70]}...\")\n",
    "    print(f\"Actual: {actual_category} | Predicted: {predicted_category} [{confidence:.2%}] {match}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel successfully trained on {len(df_filtered)} news articles\")\n",
    "print(f\"Test set accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"\\nModel is ready for predictions on new headlines.\")\n",
    "print(f\"Use: predict_headline_category('Your headline here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dbfe9-e4f6-4c6d-951b-990e51543e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì∞ WELCOME TO NEWS CATEGORY CHATBOT\n",
      "================================================================================\n",
      "\n",
      "I can predict the category of any news headline!\n",
      "\n",
      "Available categories: BUSINESS, ENTERTAINMENT, POLITICS, TECH\n",
      "\n",
      "Type 'quit', 'exit', or 'q' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìù Enter a news headline (or 'quit' to exit):  Tesla to restart Dojo supercomputer project, Musk says\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Prediction: BUSINESS\n",
      "   Confidence: 62.13%\n",
      "   Reliability: üî¥ Low (<70%)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìù Enter a news headline (or 'quit' to exit):  Tesla to restart Dojo supercomputer project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Prediction: BUSINESS\n",
      "   Confidence: 58.58%\n",
      "   Reliability: üî¥ Low (<70%)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìù Enter a news headline (or 'quit' to exit):  Tesla is reviving its Dojo supercomputer project just five months after CEO Elon Musk declared its predecessor \"an evolutionary dead end\" and disbanded the team behind it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Prediction: BUSINESS\n",
      "   Confidence: 89.07%\n",
      "   Reliability: üü¢ High (80-90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE CHATBOT - NEWS CATEGORY PREDICTOR\n",
    "# ============================================================================\n",
    "\n",
    "def run_interactive_chatbot():\n",
    "    \"\"\"\n",
    "    Interactive chatbot for predicting news headline categories.\n",
    "    Run this to chat with the model!\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üì∞ WELCOME TO NEWS CATEGORY CHATBOT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nI can predict the category of any news headline!\")\n",
    "    print(f\"\\nAvailable categories: {', '.join(model.classes_)}\")\n",
    "    print(\"\\nType 'quit', 'exit', or 'q' to end the conversation.\\n\")\n",
    "    \n",
    "    conversation_count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_headline = input(\"üìù Enter a news headline (or 'quit' to exit): \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_headline.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nüëã Thank you for using NewsBot! Goodbye!\")\n",
    "                print(f\"Total headlines processed: {conversation_count}\")\n",
    "                break\n",
    "            \n",
    "            # Validate input\n",
    "            if not user_headline:\n",
    "                print(\"‚ö†Ô∏è  Please enter a valid headline.\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_category, confidence = predict_headline_category(user_headline)\n",
    "            conversation_count += 1\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n‚úÖ Prediction: {predicted_category}\")\n",
    "            print(f\"   Confidence: {confidence:.2%}\")\n",
    "            \n",
    "            # Confidence indicator\n",
    "            if confidence > 0.9:\n",
    "                reliability = \"üü¢ Very High (>90%)\"\n",
    "            elif confidence > 0.8:\n",
    "                reliability = \"üü¢ High (80-90%)\"\n",
    "            elif confidence > 0.7:\n",
    "                reliability = \"üü° Medium (70-80%)\"\n",
    "            else:\n",
    "                reliability = \"üî¥ Low (<70%)\"\n",
    "            \n",
    "            print(f\"   Reliability: {reliability}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Please try again.\\n\")\n",
    "\n",
    "# RUN THE CHATBOT\n",
    "run_interactive_chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1c79f-af08-49ff-a080-c9edbc75b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
